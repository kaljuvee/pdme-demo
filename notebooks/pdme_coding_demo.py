# -*- coding: utf-8 -*-
"""pdme_coding_demo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/175oc0LZ-2Lr-hhxAAWgcmlN8OmAtYFXc
"""

! pip install opticonomy-pdme openai anthropic google-generativeai transformers huggingface_hub python-dotenv

"""# Overview

- The Evaluator Model is currently always assumed to be OpenAI
- Change Model 1 and Model 2 sections accordingly based on your needs and set the appropriate API keys

## References - Available Models

* [OpenAI GPT Models](https://platform.openai.com/docs/models)
* [Anthropic Claude Models](https://docs.anthropic.com/en/docs/about-claude/models)
* [Google Gemini Models](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versions)
* [HuggingFace Text Generation Models](https://huggingface.co/models?pipeline_tag=text-generation)
* [VLLM Supported Models](https://docs.vllm.ai/en/latest/models/supported_models.html#supported-models)
* [LLM Arena Leaderboard](https://chat.lmsys.org/?leaderboard)

## Notes

* This notebook assumes that various API keys are set as environment variables (eg vie .env file or otherwise)
* Some providers, such as OpenAI and Google point the latest model to a particular release (eg gpt-4o -> gpt-4o-2024-05-13) while for others such as Anthropic you have to hard code the release data / number into the model name (claude-3-5-sonnet-20240620).

# 1. Generate Boostrap Prompts (Coding)
"""

from pdme import generate_bootstrap_prompts, evaluate


seeds = { "<language>":["python", "c++"],
         "<seed>":["tic-tac-toe", "array", "sorting", "dictionary"],
         }

bootstrap_prompt_template = """Write a question asking to make a programming challenge meant to evaluate programming abilities.
The problem should be possible to solve in less than 100 lines of code for a very skilled programmer.
The problem should use the <language> language, and be realted to these seeds: <seed>, <seed>."""

# let's generate 3 bootstrap prompts
bootstrap_prompts = generate_bootstrap_prompts.create_bootstrap_prompts(template = bootstrap_prompt_template, seeds = seeds, num = 3)

# bootstrap_prompts are a list of bootsrap prompts such as:
# ["""Write a question asking to make a programming challenge meant to evaluate programming abilities.
# The problem should be possible to solve in less than 100 lines of code for a very skilled programmer.
# The problem should use the python language, and be realted to these seeds: sorting, array."""]

for item in bootstrap_prompts:
    print( item )
    print()

"""# 2. Generate Question Prompts - Any Model"""

import openai
import anthropic
import google.generativeai as genai
from dotenv import load_dotenv
import os
from transformers import AutoModelForCausalLM, AutoTokenizer
from huggingface_hub import login
import torch
import logging

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

load_dotenv()

model_name = "gpt-3.5-turbo"
# Prompt for the API key using getpass
#openai.api_key  = getpass.getpass(prompt="Enter your OpenAI API key: ")
openai_api_key = os.getenv('OPENAI_API_KEY')
anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')
google_api_key = os.getenv('GOOGLE_API_KEY')
huggingface_api_key = os.getenv('HUGGINGFACE_API_KEY')

client = openai.OpenAI(api_key = openai_api_key)

question_prompts = []

for item in bootstrap_prompts:
    response = client.chat.completions.create(
      model= model_name,
      messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": item},
      ]
    )
    question_prompts.append(response.choices[0].message.content)
for item in question_prompts:
    print( item )
    print()

"""# 3. Generate Responses

## 3.1 Define Generate Responses Function

- Currently handles top 10 models on LLM Arena which are proprietary and not available via HuggingFace (eg OpenAI, Gemini and Claude)
- TODO:Add a case to also handle also huggingface open source models
"""

def generate_responses(model_name, question_prompts):
    responses = []

    if model_name.startswith("gpt"):
        client = openai.OpenAI(api_key = openai_api_key)
        for item in question_prompts:
            response = client.chat.completions.create(
                model=model_name,
                messages=[
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": item},
                ]
            )
            responses.append(response.choices[0].message.content)

    elif model_name.startswith("claude"):
        anthropic_client = anthropic.Client(api_key=anthropic_api_key)
        for item in question_prompts:
            response = anthropic_client.messages.create(
                model=model_name,
                max_tokens=1000,
                messages=[
                    {"role": "user", "content": item}
                ]
            )
            text_response = response.content[0].text
            responses.append(text_response)

    elif model_name.startswith("gemini"):
        genai.configure(api_key=google_api_key)
        model_last_part = model_name.split('/')[-1]
        print("Generating for Gemini with: ", model_last_part)
        model = genai.GenerativeModel(model_last_part)
        for item in question_prompts:
            response = model.generate_content(item)
            responses.append(response.text)

    else:
       # open source
        huggingface_api_key = os.getenv('HUGGINGFACE_API_KEY')
        if not huggingface_api_key:
              raise ValueError("HuggingFace API key not found in environment variables.")
        login(huggingface_api_key)
        model = AutoModelForCausalLM.from_pretrained(model_name)
        tokenizer = AutoTokenizer.from_pretrained(model_name)

        for item in question_prompts:
            inputs = tokenizer.encode(item, return_tensors="pt")
            outputs = model.generate(inputs, max_length=100)
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            responses.append(response)
    return responses

"""## 3.2 Generate Responses"""

# Sample models from LLM Arena
model_list = ['claude-3-opus-20240229', 'claude-3-5-sonnet-20240620', 'gpt-4o', 'gpt-4-turbo', 'gpt-4', 'gemini-1.0-pro']

# Example usage
model_1 = "gpt-3.5-turbo"
#model_2 = 'gemini-1.0-pro'
model_2 = "Nexusflow/Athene-70B"
# Assuming client is already defined and authenticated
responses_model_1 = generate_responses(model_1, question_prompts)

for item in responses_model_1:
    print('Model 1 response: ', item)
    print()

responses_model_2 = generate_responses(model_2, question_prompts)

for item in responses_model_2:
    print('Model 2 response: ', item)
    print()

"""# 4. Evaluate Models

## 4.1 Create an Evaluator
"""

from pdme.evaluate import pdme_llm

eval_model = "gpt-3.5-turbo-instruct"
llm = pdme_llm(client, eval_model)

"""## 4.2 Define an Evaluation Template"""

evaluation_prompt_template = """<prefix><user_start>Here is a prompt:
{
    "instruction": \"""<question_full>\""",
}

Here are the outputs of the models:
[
    {
        "model": 1,
        "answer": \"""<response1>\"""
    },
    {
        "model": 2,
        "answer": \"""<response2>\"""
    }
]

Please create a dict containting the highest quality answer, i.e., produce the following output:

{
  'best_model': <model-name>
}

Please provide the response that the majority of humans would consider better.

<assistant_start>{
  'best_model': """

labels = ["1", "2"]

"""### 4.3 Score Responses"""

# Create a function that accepts evaluation_prompt_template, question prompts, responses_model_1, responses_model_2 and returns a scores dict Model 1 Scores: <>, Model 2 Scores:

def score_responses(evaluation_prompt_template, question_prompts, responses_model_1, responses_model_2):
    model_1_scores = []
    model_2_scores = []
    sum_model_1_scores = 0
    sum_model_2_scores = 0

    for i, question in enumerate(question_prompts):
        # Reverse the order of responses in the first prompt
        prompt_1 = evaluation_prompt_template.replace("<question_full>", question).replace("<response1>", responses_model_2[i]).replace("<response2>", responses_model_1[i])
        score_1 = llm.evaluate(prompt_1, ["1", "2"])

        # Use the original order in the second prompt
        prompt_2 = evaluation_prompt_template.replace("<question_full>", question).replace("<response1>", responses_model_1[i]).replace("<response2>", responses_model_2[i])
        score_2 = llm.evaluate(prompt_2, ["1", "2"])

        # Average the scores
        # Now we know exactly which score corresponds to which model
        model_1_score = (score_1[1] + score_2[0]) / 2  # Average of Model 1's scores
        model_2_score = (score_1[0] + score_2[1]) / 2  # Average of Model 2's scores

        model_1_scores.append(model_1_score)
        model_2_scores.append(model_2_score)
        sum_model_1_scores += model_1_score
        sum_model_2_scores += model_2_score

        # Append the scores to the respective lists and sum the scores
        #model_1_scores.append(avg_score[1])
        #model_2_scores.append(avg_score[0])
        #sum_model_1_scores += avg_score[1]
        #sum_model_2_scores += avg_score[0]

    # Determine the winner
    winner = "Model 1" if sum_model_1_scores > sum_model_2_scores else "Model 2"

    # Create the scores dictionary
    scores_dict = {
        "Model 1 Scores": model_1_scores,
        "Model 2 Scores": model_2_scores,
        "Model 1 Total Scores": sum_model_1_scores,
        "Model 2 Total Scores": sum_model_2_scores,
        "Winner": winner
    }

    return scores_dict

# Calling the function
scores = score_responses(evaluation_prompt_template, question_prompts, responses_model_1, responses_model_2)

# Printing the results
print("Results:", scores)

